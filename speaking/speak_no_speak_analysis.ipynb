{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2b9c81bc8f0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from math import isnan\n",
    "from torch.utils.data import DataLoader\n",
    "from dataloader import LipReadingDataset\n",
    "from speak import Speaking_conv3d_layers, Speaking_words, Speaking_model, Speaking_conv3d_layers_trimmed\n",
    "import torch\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import datetime\n",
    "from torch import nn\n",
    "from transformers import get_constant_schedule_with_warmup, get_cosine_schedule_with_warmup\n",
    "import torchvision\n",
    "from transformers import AutoTokenizer\n",
    "from main import validate\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/d/classes/project/lip_reading/speaking\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Users\\DK\\conda\\envs\\cs231\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['encoder.0.conv_block.0.weight', 'encoder.0.conv_block.0.bias', 'encoder.0.conv_block.1.weight', 'encoder.0.conv_block.1.bias', 'encoder.0.conv_block.1.running_mean', 'encoder.0.conv_block.1.running_var', 'encoder.0.conv_block.1.num_batches_tracked', 'encoder.1.conv_block.0.weight', 'encoder.1.conv_block.0.bias', 'encoder.1.conv_block.1.weight', 'encoder.1.conv_block.1.bias', 'encoder.1.conv_block.1.running_mean', 'encoder.1.conv_block.1.running_var', 'encoder.1.conv_block.1.num_batches_tracked', 'encoder.2.conv_block.0.weight', 'encoder.2.conv_block.0.bias', 'encoder.2.conv_block.1.weight', 'encoder.2.conv_block.1.bias', 'encoder.2.conv_block.1.running_mean', 'encoder.2.conv_block.1.running_var', 'encoder.2.conv_block.1.num_batches_tracked', 'encoder.3.conv_block.0.weight', 'encoder.3.conv_block.0.bias', 'encoder.3.conv_block.1.weight', 'encoder.3.conv_block.1.bias', 'encoder.3.conv_block.1.running_mean', 'encoder.3.conv_block.1.running_var', 'encoder.3.conv_block.1.num_batches_tracked', 'encoder.4.conv_block.0.weight', 'encoder.4.conv_block.0.bias', 'encoder.4.conv_block.1.weight', 'encoder.4.conv_block.1.bias', 'encoder.4.conv_block.1.running_mean', 'encoder.4.conv_block.1.running_var', 'encoder.4.conv_block.1.num_batches_tracked', 'encoder.5.conv_block.0.weight', 'encoder.5.conv_block.0.bias', 'encoder.5.conv_block.1.weight', 'encoder.5.conv_block.1.bias', 'encoder.5.conv_block.1.running_mean', 'encoder.5.conv_block.1.running_var', 'encoder.5.conv_block.1.num_batches_tracked', 'encoder.6.conv_block.0.weight', 'encoder.6.conv_block.0.bias', 'encoder.6.conv_block.1.weight', 'encoder.6.conv_block.1.bias', 'encoder.6.conv_block.1.running_mean', 'encoder.6.conv_block.1.running_var', 'encoder.6.conv_block.1.num_batches_tracked', 'encoder.7.conv_block.0.weight', 'encoder.7.conv_block.0.bias', 'encoder.7.conv_block.1.weight', 'encoder.7.conv_block.1.bias', 'encoder.7.conv_block.1.running_mean', 'encoder.7.conv_block.1.running_var', 'encoder.7.conv_block.1.num_batches_tracked', 'encoder.8.conv_block.0.weight', 'encoder.8.conv_block.0.bias', 'encoder.8.conv_block.1.weight', 'encoder.8.conv_block.1.bias', 'encoder.8.conv_block.1.running_mean', 'encoder.8.conv_block.1.running_var', 'encoder.8.conv_block.1.num_batches_tracked', 'encoder.9.conv_block.0.weight', 'encoder.9.conv_block.0.bias', 'encoder.9.conv_block.1.weight', 'encoder.9.conv_block.1.bias', 'encoder.9.conv_block.1.running_mean', 'encoder.9.conv_block.1.running_var', 'encoder.9.conv_block.1.num_batches_tracked', 'encoder.10.conv_block.0.weight', 'encoder.10.conv_block.0.bias', 'encoder.10.conv_block.1.weight', 'encoder.10.conv_block.1.bias', 'encoder.10.conv_block.1.running_mean', 'encoder.10.conv_block.1.running_var', 'encoder.10.conv_block.1.num_batches_tracked', 'linear.weight', 'linear.bias']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length_video = 125 \n",
    "tokenizer=AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "model = Speaking_conv3d_layers(512, length_video)\n",
    "weights = torch.load(\"../saved_weights/speak/best_weight_2_epochs.state\")[\"state_dict\"]\n",
    "print(list(weights.keys()))\n",
    "for key, value in list(weights.items()):  # model layer names are encoder.encoder instead of encoder.\n",
    "    if \"encoder\" in key:\n",
    "        weights.pop(key)\n",
    "        weights[f\"encoder.{key}\"] = value\n",
    "\n",
    "model.load_state_dict(weights)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples loaded: 96318\n",
      "Total samples train: 86686\n",
      "Total samples val: 9632\n"
     ]
    }
   ],
   "source": [
    "base_path = \"D:/classes/project/LRS2/extracted_data/mvlrs_v1/\"\n",
    "dataset_name=\"pretrain\"\n",
    "\n",
    "train_dataset = LipReadingDataset(directory=base_path,\n",
    "                            transform=None,\n",
    "                            length_video=length_video,\n",
    "                            mode=\"speak\",\n",
    "                            tokenizer=tokenizer,\n",
    "                            dataset=dataset_name,\n",
    "                            useOriginalDataStructure=True,\n",
    "                            h_w = (160,160)\n",
    "                            )\n",
    "print(\"Total samples loaded:\", len(train_dataset))  \n",
    "train_set, val_set = torch.utils.data.random_split(train_dataset, [int(len(train_dataset)*0.9), len(train_dataset) - int(len(train_dataset)*0.9)])\n",
    "\n",
    "train_data_loader = DataLoader(\n",
    "    train_set,\n",
    "    batch_size=1,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=False,\n",
    "    prefetch_factor=2\n",
    "    \n",
    "    )\n",
    "\n",
    "val_data_loader = DataLoader(\n",
    "    val_set,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=False,\n",
    "    prefetch_factor=2,\n",
    "    )\n",
    "print(\"Total samples train:\", len(train_data_loader))  \n",
    "print(\"Total samples val:\", len(val_data_loader))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "model.to(device)\n",
    "weight = torch.tensor([0.77, 0.23]).to(device)\n",
    "criterion = nn.CrossEntropyLoss(weight=weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9632/9632 [06:55<00:00, 23.19it/s, Ones_frac=75.41%, accuracy=77.63%, zeros_frac=24.59%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ones: 907887, zeros: 296113, total: 1204000, one_frac: 0.7540589570999146 zeors_frac: 0.24594102799892426\n",
      "total_correct: 934646 total_words: 1204000\n",
      "accuracy: 0.7762840531561461\n",
      "loss: 5224.454869732261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7762840531561461"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate(val_data_loader, model, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets trim the weights and layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "length_video = 125 \n",
    "device = \"cuda\"\n",
    "model = Speaking_conv3d_layers_trimmed(512, length_video)\n",
    "weights = torch.load(\"../trimmed_shifted.state\")[\"state_dict\"]\n",
    "model.load_state_dict(weights)\n",
    "model.to(device)\n",
    "model.to(device)\n",
    "weight = torch.tensor([0.77, 0.23]).to(device)\n",
    "criterion = nn.CrossEntropyLoss(weight=weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples loaded: 96318\n",
      "Total samples train: 86686\n",
      "Total samples val: 9632\n"
     ]
    }
   ],
   "source": [
    "base_path = \"D:/classes/project/LRS2/extracted_data/mvlrs_v1/\"\n",
    "dataset_name=\"pretrain\"\n",
    "\n",
    "train_dataset = LipReadingDataset(directory=base_path,\n",
    "                            transform=None,\n",
    "                            length_video=length_video,\n",
    "                            mode=\"speak\",\n",
    "                            tokenizer=tokenizer,\n",
    "                            dataset=dataset_name,\n",
    "                            useOriginalDataStructure=True,\n",
    "                            h_w = (160,160)\n",
    "                            )\n",
    "print(\"Total samples loaded:\", len(train_dataset))  \n",
    "train_set, val_set = torch.utils.data.random_split(train_dataset, [int(len(train_dataset)*0.9), len(train_dataset) - int(len(train_dataset)*0.9)])\n",
    "\n",
    "train_data_loader = DataLoader(\n",
    "    train_set,\n",
    "    batch_size=1,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=False,\n",
    "    prefetch_factor=2\n",
    "    \n",
    "    )\n",
    "\n",
    "val_data_loader = DataLoader(\n",
    "    val_set,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=False,\n",
    "    prefetch_factor=2,\n",
    "    )\n",
    "print(\"Total samples train:\", len(train_data_loader))  \n",
    "print(\"Total samples val:\", len(val_data_loader))  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average pooling (2) the time and H W domains to (45,4,4) makes the model suffer ataining a ~51.58% accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`self.nums_int[:, :, :, :4, :4] = self.avg(x)[:, :, :45]\n",
    "self.avg = nn.AvgPool3d((2))\n",
    "self.nums_int = torch.zeros((1, 512, 45, 5, 5)).to(\"cuda\")`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- self.avg = nn.AvgPool3d(2)\n",
    "self.nums_int = torch.zeros((1, 512, 45, 5, 5)).to(\"cuda\")\n",
    "\n",
    "self.linear = nn.Linear(d_model*3*3*length_of_video_in_frames, 2*length_of_video_in_frames) # output size is 1 or zero times the temporal dimension\n",
    "\n",
    "def forward(self, x):\n",
    "\n",
    "x = self.encoder(x)\n",
    "self.nums_int[:, :, :, :4, :4] = self.avg(x)[:, :, :45] -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9632/9632 [06:24<00:00, 25.03it/s, Ones_frac=75.52%, accuracy=51.58%, zeros_frac=24.48%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ones: 909272, zeros: 294728, total: 1204000, one_frac: 0.7552093267440796 zeors_frac: 0.2447907030582428\n",
      "total_correct: 621072 total_words: 1204000\n",
      "accuracy: 0.5158405315614618\n",
      "loss: 19001.281693696976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5158405315614618"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate(val_data_loader, model, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "self.avg(x)[:, :, :, :3, :3]\n",
    "self.avg = nn.AvgPool3d((1,2,2))\n",
    "self.nums_int = torch.zeros((1, 512, 125, 3, 3)).to(\"cuda\")\n",
    "\n",
    "48%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9632/9632 [06:26<00:00, 24.90it/s, Ones_frac=75.52%, accuracy=48.10%, zeros_frac=24.48%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ones: 909272, zeros: 294728, total: 1204000, one_frac: 0.7552093267440796 zeors_frac: 0.2447907030582428\n",
      "total_correct: 579106 total_words: 1204000\n",
      "accuracy: 0.48098504983388707\n",
      "loss: 31404.948693454266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.48098504983388707"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate(val_data_loader, model, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn.AvgPool3d((2,1,1))\n",
    "self.nums_int =    self.avg(x)[:, :, :45, :5, :5]\n",
    "self.nums_int = torch.zeros((1, 512, 45, 8, 8)).to(\"cuda\")\n",
    "\n",
    "57.9%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9632/9632 [06:27<00:00, 24.86it/s, Ones_frac=75.52%, accuracy=57.91%, zeros_frac=24.48%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ones: 909272, zeros: 294728, total: 1204000, one_frac: 0.7552093267440796 zeors_frac: 0.2447907030582428\n",
      "total_correct: 697188 total_words: 1204000\n",
      "accuracy: 0.5790598006644518\n",
      "loss: 31364.614983811975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5790598006644518"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate(val_data_loader, model, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn.maxPool3d((2,1,1))\n",
    "self.nums_int =    self.avg(x)[:, :, :45, :5, :5]\n",
    "self.nums_int = torch.zeros((1, 512, 45, 8, 8)).to(\"cuda\")\n",
    "\n",
    "same as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9632/9632 [06:28<00:00, 24.76it/s, Ones_frac=75.52%, accuracy=57.91%, zeros_frac=24.48%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ones: 909272, zeros: 294728, total: 1204000, one_frac: 0.7552093267440796 zeors_frac: 0.2447907030582428\n",
      "total_correct: 697188 total_words: 1204000\n",
      "accuracy: 0.5790598006644518\n",
      "loss: 31364.614983811975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5790598006644518"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate(val_data_loader, model, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`self.nums_int[:, :, :, :4, :4] = self.avg(x)[:, :, :45]\n",
    "self.avg = nn.maxPool3d((2))\n",
    "self.nums_int = torch.zeros((1, 512, 45, 5, 5)).to(\"cuda\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9632/9632 [06:29<00:00, 24.73it/s, Ones_frac=75.47%, accuracy=47.94%, zeros_frac=24.53%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ones: 908697, zeros: 295303, total: 1204000, one_frac: 0.7547317147254944 zeors_frac: 0.24526827037334442\n",
      "total_correct: 577221 total_words: 1204000\n",
      "accuracy: 0.4794194352159468\n",
      "loss: 42005.11504986882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4794194352159468"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate(val_data_loader, model, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs231",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
